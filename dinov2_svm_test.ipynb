{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e72fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './models/pick_place_svm.pkl'\n",
    "test_path = './data/test_images/'\n",
    "\n",
    "\n",
    "# load model\n",
    "with open(model_path, 'rb') as file:\n",
    "    clf = pickle.load(file)\n",
    "\n",
    "dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "dinov2_vits14.to(device)\n",
    "\n",
    "# image paths\n",
    "success_test_dir = os.path.join(test_path, 'success/')\n",
    "success_paths = ([os.path.join(success_test_dir, f) for f in os.listdir(success_test_dir)])\n",
    "\n",
    "failure_test_dir = os.path.join(test_path, 'failure/')\n",
    "failure_paths = ([os.path.join(failure_test_dir, f) for f in os.listdir(failure_test_dir)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3428b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = T.Compose([T.ToTensor(), \n",
    "                              T.Resize(224), \n",
    "                              T.CenterCrop(224), \n",
    "                              T.Normalize([0.5], [0.5]),\n",
    "                              T.Grayscale(num_output_channels=3),\n",
    "                              ])\n",
    "\n",
    "def load_image(img: str, transformation, augmentation=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load an image and return a tensor that can be used as an input to DINOv2.\n",
    "    \"\"\"\n",
    "    img = Image.open(img)\n",
    "\n",
    "    transformed_img = transformation(img)\n",
    "    if augmentation:\n",
    "        transformed_img = augmentation(transformed_img)\n",
    "\n",
    "    return transformed_img[:3].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f9e2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "def results(paths, expected_output=''):\n",
    "    count = 0\n",
    "    for input_file in tqdm(paths, total=len(paths)):\n",
    "        new_image = load_image(input_file, transformation)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = dinov2_vits14(new_image.to(device))\n",
    "\n",
    "            prediction = clf.predict(np.array(embedding[0].cpu()).reshape(1, -1))\n",
    "\n",
    "            # print(\"Predicted class: \" + prediction[0])\n",
    "            if prediction[0] == expected_output:\n",
    "                count += 1\n",
    "            else:\n",
    "                print(input_file)\n",
    "                plt.imshow(Image.open(input_file))\n",
    "                plt.show()\n",
    "    print(count/len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00160f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results(success_paths, 'success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78149090",
   "metadata": {},
   "outputs": [],
   "source": [
    "results(failure_paths, 'failure')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
